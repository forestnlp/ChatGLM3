{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89b89f64d8f8053d",
   "metadata": {},
   "source": [
    "# 单卡GPU 进行 ChatGLM3-6B模型 LORA 高效微调\n",
    "本 Cookbook 将带领开发者使用 `AdvertiseGen` 对 ChatGLM3-6B 数据集进行 lora微调，使其具备专业的广告生成能力。\n",
    "\n",
    "## 硬件需求\n",
    "显存：24GB\n",
    "显卡架构：安培架构（推荐）\n",
    "内存：16GB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bd9a514ed09ea6",
   "metadata": {},
   "source": [
    "## 1. 准备数据集\n",
    "我们使用 AdvertiseGen 数据集来进行微调。从 [Google Drive](https://drive.google.com/file/d/13_vf0xRTQsyneRKdD1bZIr93vBGOczrk/view?usp=sharing) 或者 [Tsinghua Cloud](https://cloud.tsinghua.edu.cn/f/b3f119a008264b1cabd1/?dl=1) 下载处理好的 AdvertiseGen 数据集，将解压后的 AdvertiseGen 目录放到本目录的 `/data/` 下, 例如。\n",
    "> /media/zr/Data/Code/ChatGLM3/finetune_demo/data/AdvertiseGen\n",
    "\n",
    "接着，运行本代码来切割数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T05:02:34.749308Z",
     "start_time": "2024-01-18T05:02:25.564458Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Union\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def _resolve_path(path: Union[str, Path]) -> Path:\n",
    "    return Path(path).expanduser().resolve()\n",
    "\n",
    "\n",
    "def _mkdir(dir_name: Union[str, Path]):\n",
    "    dir_name = _resolve_path(dir_name)\n",
    "    if not dir_name.is_dir():\n",
    "        dir_name.mkdir(parents=True, exist_ok=False)\n",
    "\n",
    "\n",
    "def convert_adgen(data_dir: Union[str, Path], save_dir: Union[str, Path]):\n",
    "    def _convert(in_file: Path, out_file: Path):\n",
    "        _mkdir(out_file.parent)\n",
    "        with open(in_file, encoding='utf-8') as fin:\n",
    "            with open(out_file, 'wt', encoding='utf-8') as fout:\n",
    "                for line in fin:\n",
    "                    dct = json.loads(line)\n",
    "                    sample = {'conversations': [{'role': 'user', 'content': dct['content']},\n",
    "                                                {'role': 'assistant', 'content': dct['summary']}]}\n",
    "                    fout.write(json.dumps(sample, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    data_dir = _resolve_path(data_dir)\n",
    "    save_dir = _resolve_path(save_dir)\n",
    "\n",
    "    train_file = data_dir / 'train.json'\n",
    "    if train_file.is_file():\n",
    "        out_file = save_dir / train_file.relative_to(data_dir)\n",
    "        _convert(train_file, out_file)\n",
    "\n",
    "    dev_file = data_dir / 'dev.json'\n",
    "    if dev_file.is_file():\n",
    "        out_file = save_dir / dev_file.relative_to(data_dir)\n",
    "        _convert(dev_file, out_file)\n",
    "\n",
    "\n",
    "convert_adgen('/root/data/AdvertiseGen', '/root/data/AdvertiseGen_fix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b7a99923349056",
   "metadata": {},
   "source": [
    "## 2. 使用命令行开始微调,我们使用 lora 进行微调\n",
    "接着，我们仅需要将配置好的参数以命令行的形式传参给程序，就可以使用命令行进行高效微调，这里将 `/media/zr/Data/Code/ChatGLM3/venv/bin/python3` 换成你的 python3 的绝对路径以保证正常运行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17c87410a24d844f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T06:44:56.043246Z",
     "start_time": "2024-01-18T05:05:28.425374Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n",
      "Loading checkpoint shards: 100%|██████████████████| 7/7 [00:02<00:00,  2.85it/s]\n",
      "trainable params: 1,949,696 || all params: 6,245,533,696 || trainable%: 0.031217444255383614\n",
      "--> Model\n",
      "\n",
      "--> model has 1.949696M params\n",
      "\n",
      "Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 114599 examples [00:00, 651676.69 examples/s]\n",
      "Setting num_proc from 16 back to 1 for the validation split to disable multiprocessing as it only contains one shard.\n",
      "Generating validation split: 1070 examples [00:00, 164169.63 examples/s]\n",
      "Setting num_proc from 16 back to 1 for the test split to disable multiprocessing as it only contains one shard.\n",
      "Generating test split: 1070 examples [00:00, 179014.97 examples/s]\n",
      "Map (num_proc=16): 100%|██████| 114599/114599 [00:02<00:00, 44333.39 examples/s]\n",
      "train_dataset: Dataset({\n",
      "    features: ['input_ids', 'labels'],\n",
      "    num_rows: 114599\n",
      "})\n",
      "Map (num_proc=16): 100%|████████████| 1070/1070 [00:01<00:00, 878.35 examples/s]\n",
      "val_dataset: Dataset({\n",
      "    features: ['input_ids', 'output_ids'],\n",
      "    num_rows: 1070\n",
      "})\n",
      "Map (num_proc=16): 100%|████████████| 1070/1070 [00:01<00:00, 848.60 examples/s]\n",
      "test_dataset: Dataset({\n",
      "    features: ['input_ids', 'output_ids'],\n",
      "    num_rows: 1070\n",
      "})\n",
      "--> Sanity check\n",
      "           '[gMASK]': 64790 -> -100\n",
      "               'sop': 64792 -> -100\n",
      "          '<|user|>': 64795 -> -100\n",
      "                  '': 30910 -> -100\n",
      "                '\\n': 13 -> -100\n",
      "                  '': 30910 -> -100\n",
      "                '类型': 33467 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                 '版': 55090 -> -100\n",
      "                 '型': 54888 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '宽松': 40833 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                '风格': 32799 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '性感': 40589 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                '图案': 37505 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '线条': 37216 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "                 '型': 54888 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                 '阔': 56529 -> -100\n",
      "                 '腿': 56158 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "     '<|assistant|>': 64796 -> -100\n",
      "                  '': 30910 -> 30910\n",
      "                '\\n': 13 -> 13\n",
      "                  '': 30910 -> 30910\n",
      "                '宽松': 40833 -> 40833\n",
      "                 '的': 54530 -> 54530\n",
      "                 '阔': 56529 -> 56529\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '裤': 56532 -> 56532\n",
      "                 '这': 54551 -> 54551\n",
      "                '两年': 33808 -> 33808\n",
      "                '真的': 32041 -> 32041\n",
      "                 '吸': 55360 -> 55360\n",
      "                 '粉': 55486 -> 55486\n",
      "                '不少': 32138 -> 32138\n",
      "                 '，': 31123 -> 31123\n",
      "                '明星': 32943 -> 32943\n",
      "                '时尚': 33481 -> 33481\n",
      "                 '达': 54880 -> 54880\n",
      "                '人的': 31664 -> 31664\n",
      "                '心头': 46565 -> 46565\n",
      "                 '爱': 54799 -> 54799\n",
      "                 '。': 31155 -> 31155\n",
      "                '毕竟': 33051 -> 33051\n",
      "                 '好': 54591 -> 54591\n",
      "                 '穿': 55432 -> 55432\n",
      "                '时尚': 33481 -> 33481\n",
      "                 '，': 31123 -> 31123\n",
      "                 '谁': 55622 -> 55622\n",
      "                '都能': 32904 -> 32904\n",
      "                 '穿': 55432 -> 55432\n",
      "                 '出': 54557 -> 54557\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '长': 54625 -> 54625\n",
      "                 '2': 30943 -> 30943\n",
      "                 '米': 55055 -> 55055\n",
      "               '的效果': 35590 -> 35590\n",
      "                '宽松': 40833 -> 40833\n",
      "                 '的': 54530 -> 54530\n",
      "                 '裤': 56532 -> 56532\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '，': 31123 -> 31123\n",
      "               '当然是': 48466 -> 48466\n",
      "                 '遮': 57148 -> 57148\n",
      "                 '肉': 55343 -> 55343\n",
      "                 '小': 54603 -> 54603\n",
      "                '能手': 49355 -> 49355\n",
      "                 '啊': 55674 -> 55674\n",
      "                 '。': 31155 -> 31155\n",
      "                '上身': 51605 -> 51605\n",
      "                 '随': 55119 -> 55119\n",
      "                 '性': 54642 -> 54642\n",
      "                '自然': 31799 -> 31799\n",
      "                 '不': 54535 -> 54535\n",
      "                 '拘': 57036 -> 57036\n",
      "                 '束': 55625 -> 55625\n",
      "                 '，': 31123 -> 31123\n",
      "                '面料': 46839 -> 46839\n",
      "                 '亲': 55113 -> 55113\n",
      "                 '肤': 56089 -> 56089\n",
      "                '舒适': 33894 -> 33894\n",
      "                 '贴': 55778 -> 55778\n",
      "                '身体': 31902 -> 31902\n",
      "                 '验': 55017 -> 55017\n",
      "                 '感': 54706 -> 54706\n",
      "                 '棒': 56382 -> 56382\n",
      "                 '棒': 56382 -> 56382\n",
      "                 '哒': 59230 -> 59230\n",
      "                 '。': 31155 -> 31155\n",
      "                 '系': 54712 -> 54712\n",
      "                 '带': 54882 -> 54882\n",
      "                '部分': 31726 -> 31726\n",
      "                '增加': 31917 -> 31917\n",
      "                '设计': 31735 -> 31735\n",
      "                '看点': 45032 -> 45032\n",
      "                 '，': 31123 -> 31123\n",
      "                 '还': 54656 -> 54656\n",
      "                 '让': 54772 -> 54772\n",
      "                '单品': 46539 -> 46539\n",
      "               '的设计': 34481 -> 34481\n",
      "                 '感': 54706 -> 54706\n",
      "                '更强': 43084 -> 43084\n",
      "                 '。': 31155 -> 31155\n",
      "                '腿部': 46799 -> 46799\n",
      "                '线条': 37216 -> 37216\n",
      "                 '若': 55351 -> 55351\n",
      "                 '隐': 55733 -> 55733\n",
      "                 '若': 55351 -> 55351\n",
      "                 '现': 54600 -> 54600\n",
      "                 '的': 54530 -> 54530\n",
      "                 '，': 31123 -> 31123\n",
      "                '性感': 40589 -> 40589\n",
      "                 '撩': 58521 -> 58521\n",
      "                 '人': 54533 -> 54533\n",
      "                 '。': 31155 -> 31155\n",
      "                '颜色': 33692 -> 33692\n",
      "                 '敲': 57004 -> 57004\n",
      "                '温柔': 34678 -> 34678\n",
      "                 '的': 54530 -> 54530\n",
      "                 '，': 31123 -> 31123\n",
      "                 '与': 54619 -> 54619\n",
      "                '裤子': 44722 -> 44722\n",
      "                '本身': 32754 -> 32754\n",
      "                 '所': 54626 -> 54626\n",
      "                '呈现': 33169 -> 33169\n",
      "               '的风格': 48084 -> 48084\n",
      "                '有点': 33149 -> 33149\n",
      "                 '反': 54955 -> 54955\n",
      "                 '差': 55342 -> 55342\n",
      "                 '萌': 56842 -> 56842\n",
      "                 '。': 31155 -> 31155\n",
      "                  '': 2 -> 2\n",
      "/root/miniconda3/envs/llm/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "***** Running training *****\n",
      "  Num examples = 114,599\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10,000\n",
      "  Number of trainable parameters = 1,949,696\n",
      "{'loss': 4.8277, 'grad_norm': 2.3164756298065186, 'learning_rate': 4.995e-05, 'epoch': 0.0}\n",
      "{'loss': 4.5945, 'grad_norm': 3.2689478397369385, 'learning_rate': 4.99e-05, 'epoch': 0.0}\n",
      "{'loss': 4.4773, 'grad_norm': 3.0741469860076904, 'learning_rate': 4.9850000000000006e-05, 'epoch': 0.0}\n",
      "{'loss': 4.1137, 'grad_norm': 3.427344799041748, 'learning_rate': 4.9800000000000004e-05, 'epoch': 0.0}\n",
      "{'loss': 4.1135, 'grad_norm': 2.7293148040771484, 'learning_rate': 4.975e-05, 'epoch': 0.0}\n",
      "{'loss': 3.8674, 'grad_norm': 2.9590158462524414, 'learning_rate': 4.97e-05, 'epoch': 0.0}\n",
      "{'loss': 3.8395, 'grad_norm': 2.90794038772583, 'learning_rate': 4.965e-05, 'epoch': 0.0}\n",
      "{'loss': 3.7439, 'grad_norm': 2.9776217937469482, 'learning_rate': 4.96e-05, 'epoch': 0.0}\n",
      "{'loss': 3.6354, 'grad_norm': 3.242093563079834, 'learning_rate': 4.9550000000000005e-05, 'epoch': 0.0}\n",
      "{'loss': 3.7152, 'grad_norm': 3.4600110054016113, 'learning_rate': 4.9500000000000004e-05, 'epoch': 0.0}\n",
      "{'loss': 3.6641, 'grad_norm': 3.6602554321289062, 'learning_rate': 4.945e-05, 'epoch': 0.0}\n",
      "{'loss': 3.8459, 'grad_norm': 3.9358937740325928, 'learning_rate': 4.94e-05, 'epoch': 0.0}\n",
      "{'loss': 3.61, 'grad_norm': 3.524291515350342, 'learning_rate': 4.935e-05, 'epoch': 0.0}\n",
      "{'loss': 3.7246, 'grad_norm': 4.48261022567749, 'learning_rate': 4.93e-05, 'epoch': 0.0}\n",
      "{'loss': 3.6771, 'grad_norm': 3.712878704071045, 'learning_rate': 4.9250000000000004e-05, 'epoch': 0.01}\n",
      "{'loss': 3.7381, 'grad_norm': 4.019293785095215, 'learning_rate': 4.92e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5705, 'grad_norm': 4.1636433601379395, 'learning_rate': 4.915e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5727, 'grad_norm': 4.370540142059326, 'learning_rate': 4.91e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5439, 'grad_norm': 4.860013484954834, 'learning_rate': 4.905e-05, 'epoch': 0.01}\n",
      "{'loss': 3.574, 'grad_norm': 4.5925164222717285, 'learning_rate': 4.9e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5496, 'grad_norm': 4.997010707855225, 'learning_rate': 4.8950000000000004e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6439, 'grad_norm': 4.101832389831543, 'learning_rate': 4.89e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6092, 'grad_norm': 4.847161293029785, 'learning_rate': 4.885e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5092, 'grad_norm': 4.605795860290527, 'learning_rate': 4.88e-05, 'epoch': 0.01}\n",
      "{'loss': 3.476, 'grad_norm': 5.432920932769775, 'learning_rate': 4.875e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5994, 'grad_norm': 5.326334476470947, 'learning_rate': 4.87e-05, 'epoch': 0.01}\n",
      "{'loss': 3.543, 'grad_norm': 5.422689914703369, 'learning_rate': 4.8650000000000003e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6125, 'grad_norm': 4.618794918060303, 'learning_rate': 4.86e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6285, 'grad_norm': 4.843501567840576, 'learning_rate': 4.855e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5365, 'grad_norm': 5.949028491973877, 'learning_rate': 4.85e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4627, 'grad_norm': 5.422054767608643, 'learning_rate': 4.845e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6035, 'grad_norm': 5.802596092224121, 'learning_rate': 4.8400000000000004e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4131, 'grad_norm': 5.321938991546631, 'learning_rate': 4.835e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4891, 'grad_norm': 5.436460018157959, 'learning_rate': 4.83e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5156, 'grad_norm': 5.633511543273926, 'learning_rate': 4.825e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5721, 'grad_norm': 5.2999958992004395, 'learning_rate': 4.82e-05, 'epoch': 0.01}\n",
      "{'loss': 3.358, 'grad_norm': 4.891287326812744, 'learning_rate': 4.815e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5283, 'grad_norm': 5.206904888153076, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5193, 'grad_norm': 5.249857425689697, 'learning_rate': 4.805e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4691, 'grad_norm': 5.623113632202148, 'learning_rate': 4.8e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6924, 'grad_norm': 5.523134708404541, 'learning_rate': 4.795e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4941, 'grad_norm': 5.009390354156494, 'learning_rate': 4.79e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6242, 'grad_norm': 5.672807216644287, 'learning_rate': 4.785e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4154, 'grad_norm': 6.603055477142334, 'learning_rate': 4.78e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4092, 'grad_norm': 6.181520462036133, 'learning_rate': 4.775e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4242, 'grad_norm': 5.616281986236572, 'learning_rate': 4.77e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5312, 'grad_norm': 5.6503143310546875, 'learning_rate': 4.765e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4463, 'grad_norm': 7.143671989440918, 'learning_rate': 4.76e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4625, 'grad_norm': 5.941557884216309, 'learning_rate': 4.755e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5574, 'grad_norm': 5.984643459320068, 'learning_rate': 4.75e-05, 'epoch': 0.02}\n",
      "{'loss': 3.3189, 'grad_norm': 5.886758804321289, 'learning_rate': 4.745e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5434, 'grad_norm': 6.719662189483643, 'learning_rate': 4.74e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5842, 'grad_norm': 5.984611988067627, 'learning_rate': 4.735e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4836, 'grad_norm': 5.411402225494385, 'learning_rate': 4.73e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5201, 'grad_norm': 5.370450973510742, 'learning_rate': 4.7249999999999997e-05, 'epoch': 0.02}\n",
      "{'loss': 3.6451, 'grad_norm': 5.775656700134277, 'learning_rate': 4.72e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4932, 'grad_norm': 5.807136058807373, 'learning_rate': 4.715e-05, 'epoch': 0.02}\n",
      "{'loss': 3.3715, 'grad_norm': 5.648929119110107, 'learning_rate': 4.71e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4189, 'grad_norm': 6.2138261795043945, 'learning_rate': 4.705e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4914, 'grad_norm': 6.53387975692749, 'learning_rate': 4.7e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4365, 'grad_norm': 6.200248718261719, 'learning_rate': 4.695e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4564, 'grad_norm': 6.59063720703125, 'learning_rate': 4.69e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4455, 'grad_norm': 5.921092510223389, 'learning_rate': 4.685000000000001e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4527, 'grad_norm': 6.354887008666992, 'learning_rate': 4.6800000000000006e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5299, 'grad_norm': 5.92228889465332, 'learning_rate': 4.6750000000000005e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4783, 'grad_norm': 6.457259654998779, 'learning_rate': 4.6700000000000003e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5395, 'grad_norm': 6.161995887756348, 'learning_rate': 4.665e-05, 'epoch': 0.02}\n",
      "{'loss': 3.3002, 'grad_norm': 7.1371965408325195, 'learning_rate': 4.660000000000001e-05, 'epoch': 0.02}\n",
      "{'loss': 3.3965, 'grad_norm': 6.732559680938721, 'learning_rate': 4.655000000000001e-05, 'epoch': 0.02}\n",
      "{'loss': 3.3553, 'grad_norm': 6.202805995941162, 'learning_rate': 4.6500000000000005e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4969, 'grad_norm': 6.926247596740723, 'learning_rate': 4.6450000000000004e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5295, 'grad_norm': 6.7087578773498535, 'learning_rate': 4.64e-05, 'epoch': 0.03}\n",
      "{'loss': 3.243, 'grad_norm': 6.8626275062561035, 'learning_rate': 4.635e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5717, 'grad_norm': 5.763248443603516, 'learning_rate': 4.630000000000001e-05, 'epoch': 0.03}\n",
      "{'loss': 3.3973, 'grad_norm': 6.448834419250488, 'learning_rate': 4.6250000000000006e-05, 'epoch': 0.03}\n",
      "{'loss': 3.477, 'grad_norm': 6.2817840576171875, 'learning_rate': 4.6200000000000005e-05, 'epoch': 0.03}\n",
      "{'loss': 3.6217, 'grad_norm': 6.451343059539795, 'learning_rate': 4.6150000000000004e-05, 'epoch': 0.03}\n",
      "{'loss': 3.473, 'grad_norm': 6.356448173522949, 'learning_rate': 4.61e-05, 'epoch': 0.03}\n",
      "{'loss': 3.3238, 'grad_norm': 6.49345588684082, 'learning_rate': 4.605e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5449, 'grad_norm': 6.955912113189697, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.03}\n",
      "{'loss': 3.2898, 'grad_norm': 6.715036869049072, 'learning_rate': 4.5950000000000006e-05, 'epoch': 0.03}\n",
      "{'loss': 3.3607, 'grad_norm': 6.463832378387451, 'learning_rate': 4.5900000000000004e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4604, 'grad_norm': 7.203978538513184, 'learning_rate': 4.585e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4045, 'grad_norm': 6.257131576538086, 'learning_rate': 4.58e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5072, 'grad_norm': 6.176370620727539, 'learning_rate': 4.575e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5312, 'grad_norm': 6.101597309112549, 'learning_rate': 4.5700000000000006e-05, 'epoch': 0.03}\n",
      "{'loss': 3.2924, 'grad_norm': 7.200106143951416, 'learning_rate': 4.5650000000000005e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4873, 'grad_norm': 6.676577568054199, 'learning_rate': 4.5600000000000004e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4531, 'grad_norm': 7.306870460510254, 'learning_rate': 4.555e-05, 'epoch': 0.03}\n",
      "{'loss': 3.2666, 'grad_norm': 7.535669803619385, 'learning_rate': 4.55e-05, 'epoch': 0.03}\n",
      "{'loss': 3.458, 'grad_norm': 7.582947254180908, 'learning_rate': 4.545000000000001e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4203, 'grad_norm': 6.873610019683838, 'learning_rate': 4.5400000000000006e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4604, 'grad_norm': 7.356423854827881, 'learning_rate': 4.5350000000000005e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5668, 'grad_norm': 7.138205528259277, 'learning_rate': 4.53e-05, 'epoch': 0.03}\n",
      "{'loss': 3.3633, 'grad_norm': 6.353392124176025, 'learning_rate': 4.525e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4322, 'grad_norm': 7.955219745635986, 'learning_rate': 4.52e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5309, 'grad_norm': 5.805070400238037, 'learning_rate': 4.5150000000000006e-05, 'epoch': 0.03}\n",
      "{'loss': 3.324, 'grad_norm': 6.900696754455566, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4588, 'grad_norm': 7.275300025939941, 'learning_rate': 4.5050000000000004e-05, 'epoch': 0.03}\n",
      "{'loss': 3.3953, 'grad_norm': 8.08025074005127, 'learning_rate': 4.5e-05, 'epoch': 0.03}\n",
      " 10%|███▌                                | 1000/10000 [09:31<1:28:43,  1.69it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:01<00:01,  1.07it/s]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:03<00:01,  1.19s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:05<00:00,  1.41s/it]\u001b[ABuilding prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.668 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "                                                                                \n",
      "\u001b[A{'eval_rouge-1': 31.424701999999996, 'eval_rouge-2': 6.764996, 'eval_rouge-l': 25.448224, 'eval_bleu-4': 0.03251671478052932, 'eval_runtime': 9.3702, 'eval_samples_per_second': 5.336, 'eval_steps_per_second': 0.427, 'epoch': 0.03}\n",
      " 10%|███▌                                | 1000/10000 [09:41<1:28:43,  1.69it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:06<00:00,  1.41s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-1000\n",
      "/root/miniconda3/envs/llm/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /root/autodl-tmp/chatglm3-6b/ - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "tokenizer config file saved in ./output/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1000/special_tokens_map.json\n",
      "{'loss': 3.4465, 'grad_norm': 6.992892265319824, 'learning_rate': 4.495e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4543, 'grad_norm': 7.309525012969971, 'learning_rate': 4.49e-05, 'epoch': 0.04}\n",
      "{'loss': 3.6506, 'grad_norm': 7.967966556549072, 'learning_rate': 4.4850000000000006e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4006, 'grad_norm': 6.489123344421387, 'learning_rate': 4.4800000000000005e-05, 'epoch': 0.04}\n",
      "{'loss': 3.39, 'grad_norm': 8.498688697814941, 'learning_rate': 4.4750000000000004e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3555, 'grad_norm': 7.820268154144287, 'learning_rate': 4.47e-05, 'epoch': 0.04}\n",
      "{'loss': 3.393, 'grad_norm': 7.027451992034912, 'learning_rate': 4.465e-05, 'epoch': 0.04}\n",
      "{'loss': 3.457, 'grad_norm': 7.154455184936523, 'learning_rate': 4.46e-05, 'epoch': 0.04}\n",
      "{'loss': 3.5303, 'grad_norm': 6.929489612579346, 'learning_rate': 4.4550000000000005e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4686, 'grad_norm': 6.54254150390625, 'learning_rate': 4.4500000000000004e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3434, 'grad_norm': 6.844843864440918, 'learning_rate': 4.445e-05, 'epoch': 0.04}\n",
      "{'loss': 3.5244, 'grad_norm': 7.699040412902832, 'learning_rate': 4.44e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4314, 'grad_norm': 7.214831352233887, 'learning_rate': 4.435e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3602, 'grad_norm': 7.910898685455322, 'learning_rate': 4.43e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3217, 'grad_norm': 7.585384368896484, 'learning_rate': 4.4250000000000005e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3564, 'grad_norm': 7.199276447296143, 'learning_rate': 4.4200000000000004e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4494, 'grad_norm': 6.589318752288818, 'learning_rate': 4.415e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4717, 'grad_norm': 6.184809684753418, 'learning_rate': 4.41e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3596, 'grad_norm': 6.62855339050293, 'learning_rate': 4.405e-05, 'epoch': 0.04}\n",
      "{'loss': 3.408, 'grad_norm': 6.298327922821045, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.04}\n",
      "{'loss': 3.243, 'grad_norm': 6.410569667816162, 'learning_rate': 4.3950000000000004e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3361, 'grad_norm': 7.133035182952881, 'learning_rate': 4.39e-05, 'epoch': 0.04}\n",
      "{'loss': 3.375, 'grad_norm': 7.324579238891602, 'learning_rate': 4.385e-05, 'epoch': 0.04}\n",
      "{'loss': 3.376, 'grad_norm': 7.902264595031738, 'learning_rate': 4.38e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4428, 'grad_norm': 6.684749603271484, 'learning_rate': 4.375e-05, 'epoch': 0.04}\n",
      "{'loss': 3.285, 'grad_norm': 7.3819355964660645, 'learning_rate': 4.3700000000000005e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4604, 'grad_norm': 6.91516637802124, 'learning_rate': 4.3650000000000004e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3258, 'grad_norm': 6.838498592376709, 'learning_rate': 4.36e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3828, 'grad_norm': 6.555349349975586, 'learning_rate': 4.355e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4834, 'grad_norm': 7.246053695678711, 'learning_rate': 4.35e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4605, 'grad_norm': 6.910580158233643, 'learning_rate': 4.345e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4568, 'grad_norm': 6.481893062591553, 'learning_rate': 4.3400000000000005e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3943, 'grad_norm': 10.308812141418457, 'learning_rate': 4.335e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3055, 'grad_norm': 7.3481645584106445, 'learning_rate': 4.33e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3535, 'grad_norm': 7.264320373535156, 'learning_rate': 4.325e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3002, 'grad_norm': 7.759542942047119, 'learning_rate': 4.32e-05, 'epoch': 0.05}\n",
      "{'loss': 3.5141, 'grad_norm': 7.0359086990356445, 'learning_rate': 4.315e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3844, 'grad_norm': 7.0383710861206055, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3627, 'grad_norm': 6.907633304595947, 'learning_rate': 4.305e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4139, 'grad_norm': 6.474017143249512, 'learning_rate': 4.3e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3408, 'grad_norm': 7.199398040771484, 'learning_rate': 4.295e-05, 'epoch': 0.05}\n",
      "{'loss': 3.2596, 'grad_norm': 7.3876166343688965, 'learning_rate': 4.29e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3828, 'grad_norm': 7.578176021575928, 'learning_rate': 4.285e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3508, 'grad_norm': 6.895132064819336, 'learning_rate': 4.2800000000000004e-05, 'epoch': 0.05}\n",
      "{'loss': 3.2574, 'grad_norm': 6.666514873504639, 'learning_rate': 4.275e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3885, 'grad_norm': 7.152793884277344, 'learning_rate': 4.27e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4436, 'grad_norm': 9.919925689697266, 'learning_rate': 4.265e-05, 'epoch': 0.05}\n",
      "{'loss': 3.2951, 'grad_norm': 6.487420558929443, 'learning_rate': 4.26e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4338, 'grad_norm': 7.2529215812683105, 'learning_rate': 4.2550000000000004e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4545, 'grad_norm': 6.636380672454834, 'learning_rate': 4.25e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3391, 'grad_norm': 6.557590007781982, 'learning_rate': 4.245e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3848, 'grad_norm': 8.180827140808105, 'learning_rate': 4.24e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4332, 'grad_norm': 7.90183687210083, 'learning_rate': 4.235e-05, 'epoch': 0.05}\n",
      "{'loss': 3.402, 'grad_norm': 6.649133682250977, 'learning_rate': 4.23e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4926, 'grad_norm': 7.023233413696289, 'learning_rate': 4.2250000000000004e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4027, 'grad_norm': 8.105587005615234, 'learning_rate': 4.22e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4615, 'grad_norm': 7.514811038970947, 'learning_rate': 4.215e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4281, 'grad_norm': 7.4576592445373535, 'learning_rate': 4.21e-05, 'epoch': 0.06}\n",
      "{'loss': 3.5133, 'grad_norm': 9.06998348236084, 'learning_rate': 4.205e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3932, 'grad_norm': 6.8832783699035645, 'learning_rate': 4.2e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3664, 'grad_norm': 7.648075103759766, 'learning_rate': 4.195e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3646, 'grad_norm': 7.913460731506348, 'learning_rate': 4.19e-05, 'epoch': 0.06}\n",
      "{'loss': 3.4662, 'grad_norm': 6.776243686676025, 'learning_rate': 4.185e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3145, 'grad_norm': 7.764288902282715, 'learning_rate': 4.18e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3705, 'grad_norm': 7.2128400802612305, 'learning_rate': 4.175e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3021, 'grad_norm': 6.693244934082031, 'learning_rate': 4.17e-05, 'epoch': 0.06}\n",
      "{'loss': 3.4752, 'grad_norm': 8.131375312805176, 'learning_rate': 4.165e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3674, 'grad_norm': 6.92908239364624, 'learning_rate': 4.16e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3748, 'grad_norm': 7.098452568054199, 'learning_rate': 4.155e-05, 'epoch': 0.06}\n",
      "{'loss': 3.5123, 'grad_norm': 6.593879222869873, 'learning_rate': 4.15e-05, 'epoch': 0.06}\n",
      "{'loss': 3.457, 'grad_norm': 7.380583763122559, 'learning_rate': 4.145e-05, 'epoch': 0.06}\n",
      "{'loss': 3.5021, 'grad_norm': 7.156365871429443, 'learning_rate': 4.14e-05, 'epoch': 0.06}\n",
      "{'loss': 3.4049, 'grad_norm': 7.346520900726318, 'learning_rate': 4.135e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3904, 'grad_norm': 6.919864177703857, 'learning_rate': 4.13e-05, 'epoch': 0.06}\n",
      "{'loss': 3.4648, 'grad_norm': 7.31802225112915, 'learning_rate': 4.125e-05, 'epoch': 0.06}\n",
      "{'loss': 3.434, 'grad_norm': 7.43524169921875, 'learning_rate': 4.12e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3596, 'grad_norm': 8.340980529785156, 'learning_rate': 4.115e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3469, 'grad_norm': 7.78460693359375, 'learning_rate': 4.11e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3937, 'grad_norm': 7.852340221405029, 'learning_rate': 4.105e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3348, 'grad_norm': 7.345805644989014, 'learning_rate': 4.1e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3781, 'grad_norm': 8.6171875, 'learning_rate': 4.095e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3314, 'grad_norm': 7.308595657348633, 'learning_rate': 4.09e-05, 'epoch': 0.06}\n",
      "{'loss': 3.574, 'grad_norm': 7.482843399047852, 'learning_rate': 4.085e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3514, 'grad_norm': 8.128453254699707, 'learning_rate': 4.08e-05, 'epoch': 0.06}\n",
      "{'loss': 3.493, 'grad_norm': 8.428834915161133, 'learning_rate': 4.075e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3834, 'grad_norm': 7.247756004333496, 'learning_rate': 4.07e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3166, 'grad_norm': 7.699573040008545, 'learning_rate': 4.065e-05, 'epoch': 0.07}\n",
      "{'loss': 3.3053, 'grad_norm': 7.052743911743164, 'learning_rate': 4.0600000000000004e-05, 'epoch': 0.07}\n",
      "{'loss': 3.3926, 'grad_norm': 6.749587059020996, 'learning_rate': 4.055e-05, 'epoch': 0.07}\n",
      "{'loss': 3.3697, 'grad_norm': 7.4547505378723145, 'learning_rate': 4.05e-05, 'epoch': 0.07}\n",
      "{'loss': 3.3828, 'grad_norm': 7.760422229766846, 'learning_rate': 4.045000000000001e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4748, 'grad_norm': 6.94553279876709, 'learning_rate': 4.0400000000000006e-05, 'epoch': 0.07}\n",
      "{'loss': 3.2768, 'grad_norm': 7.181018829345703, 'learning_rate': 4.0350000000000005e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4998, 'grad_norm': 7.032775402069092, 'learning_rate': 4.0300000000000004e-05, 'epoch': 0.07}\n",
      "{'loss': 3.3627, 'grad_norm': 6.602560997009277, 'learning_rate': 4.025e-05, 'epoch': 0.07}\n",
      "{'loss': 3.2846, 'grad_norm': 8.313295364379883, 'learning_rate': 4.02e-05, 'epoch': 0.07}\n",
      "{'loss': 3.3668, 'grad_norm': 7.198538303375244, 'learning_rate': 4.015000000000001e-05, 'epoch': 0.07}\n",
      "{'loss': 3.2244, 'grad_norm': 7.373408317565918, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4055, 'grad_norm': 6.724765300750732, 'learning_rate': 4.0050000000000004e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4586, 'grad_norm': 7.707154750823975, 'learning_rate': 4e-05, 'epoch': 0.07}\n",
      " 20%|███████▏                            | 2000/10000 [19:08<1:13:37,  1.81it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:05<00:05,  2.87s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:11<00:04,  4.02s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 29.408194, 'eval_rouge-2': 6.410736000000001, 'eval_rouge-l': 22.141106, 'eval_bleu-4': 0.030601816249829526, 'eval_runtime': 23.2981, 'eval_samples_per_second': 2.146, 'eval_steps_per_second': 0.172, 'epoch': 0.07}\n",
      " 20%|███████▏                            | 2000/10000 [19:31<1:13:37,  1.81it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:16<00:00,  4.43s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-2000\n",
      "/root/miniconda3/envs/llm/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /root/autodl-tmp/chatglm3-6b/ - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "tokenizer config file saved in ./output/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2000/special_tokens_map.json\n",
      " 20%|███████▏                            | 2009/10000 [19:36<2:07:18,  1.05it/s]^C\n",
      "\n",
      "\u001b[31mAborted.\u001b[0m\n",
      " 20%|███████▏                            | 2009/10000 [19:37<1:18:02,  1.71it/s]\n"
     ]
    }
   ],
   "source": [
    "!/root/miniconda3/envs/llm/bin/python3 finetune_hf.py  /root/data/AdvertiseGen_fix  /root/autodl-tmp/chatglm3-6b/  configs/lora.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9418f6c5c264601",
   "metadata": {},
   "source": [
    "## 3. 使用微调的数据集进行推理\n",
    "在完成微调任务之后，我们可以查看到 `output` 文件夹下多了很多个`checkpoint-*`的文件夹，这些文件夹代表了训练的轮数。\n",
    "我们选择最后一轮的微调权重，并使用inference进行导入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f22b735175e1c0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T07:03:19.390123Z",
     "start_time": "2024-01-18T07:03:19.246666Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint-1000  checkpoint-2000\n"
     ]
    }
   ],
   "source": [
    "!ls output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5060015c24e97ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T07:08:13.616364Z",
     "start_time": "2024-01-18T07:07:07.346906Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 7/7 [00:00<00:00, 16.73it/s]\n",
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n",
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n",
      "\u001b[31m╭─\u001b[0m\u001b[31m────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/root/ChatGLM3/finetune_demo/\u001b[0m\u001b[1;33minference_hf.py\u001b[0m:\u001b[94m51\u001b[0m in \u001b[92mmain\u001b[0m                      \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m48 \u001b[0m\u001b[2m│   │   \u001b[0mprompt: Annotated[\u001b[96mstr\u001b[0m, typer.Option(help=\u001b[33m'\u001b[0m\u001b[33m'\u001b[0m)],                  \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m49 \u001b[0m):                                                                      \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m50 \u001b[0m\u001b[2m│   \u001b[0mmodel, tokenizer = load_model_and_tokenizer(model_dir)              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m51 \u001b[2m│   \u001b[0mresponse, _ = model.chat(tokenizer, prompt)                         \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m52 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(response)                                                     \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m53 \u001b[0m                                                                        \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m54 \u001b[0m                                                                        \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/root/miniconda3/envs/llm/lib/python3.11/site-packages/torch/utils/\u001b[0m\u001b[1;33m_contextl\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[1;33mib.py\u001b[0m:\u001b[94m115\u001b[0m in \u001b[92mdecorate_context\u001b[0m                                                \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m112 \u001b[0m\u001b[2m│   \u001b[0m\u001b[1;95m@functools\u001b[0m.wraps(func)                                             \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m113 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mdecorate_context\u001b[0m(*args, **kwargs):                             \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m114 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mwith\u001b[0m ctx_factory():                                            \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m115 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m func(*args, **kwargs)                               \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m116 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m117 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m decorate_context                                            \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m118 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/root/.cache/huggingface/modules/transformers_modules/\u001b[0m\u001b[1;33mmodeling_chatglm.py\u001b[0m:\u001b[94m10\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[94m42\u001b[0m in \u001b[92mchat\u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1039 \u001b[0m\u001b[2m│   │   \u001b[0minputs = inputs.to(\u001b[96mself\u001b[0m.device)                               \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1040 \u001b[0m\u001b[2m│   │   \u001b[0meos_token_id = [tokenizer.eos_token_id, tokenizer.get_command \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1041 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0mtokenizer.get_command(\u001b[33m\"\u001b[0m\u001b[33m<|observation|>\u001b[0m\u001b[33m\"\u001b[0m)]     \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1042 \u001b[2m│   │   \u001b[0moutputs = \u001b[96mself\u001b[0m.generate(**inputs, **gen_kwargs, eos_token_id= \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1043 \u001b[0m\u001b[2m│   │   \u001b[0moutputs = outputs.tolist()[\u001b[94m0\u001b[0m][\u001b[96mlen\u001b[0m(inputs[\u001b[33m\"\u001b[0m\u001b[33minput_ids\u001b[0m\u001b[33m\"\u001b[0m][\u001b[94m0\u001b[0m]):-\u001b[94m1\u001b[0m] \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1044 \u001b[0m\u001b[2m│   │   \u001b[0mresponse = tokenizer.decode(outputs)                          \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1045 \u001b[0m\u001b[2m│   │   \u001b[0mhistory.append({\u001b[33m\"\u001b[0m\u001b[33mrole\u001b[0m\u001b[33m\"\u001b[0m: role, \u001b[33m\"\u001b[0m\u001b[33mcontent\u001b[0m\u001b[33m\"\u001b[0m: query})              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/root/miniconda3/envs/llm/lib/python3.11/site-packages/torch/utils/\u001b[0m\u001b[1;33m_contextl\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[1;33mib.py\u001b[0m:\u001b[94m115\u001b[0m in \u001b[92mdecorate_context\u001b[0m                                                \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m112 \u001b[0m\u001b[2m│   \u001b[0m\u001b[1;95m@functools\u001b[0m.wraps(func)                                             \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m113 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mdecorate_context\u001b[0m(*args, **kwargs):                             \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m114 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mwith\u001b[0m ctx_factory():                                            \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m115 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m func(*args, **kwargs)                               \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m116 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m117 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m decorate_context                                            \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m118 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/root/miniconda3/envs/llm/lib/python3.11/site-packages/transformers/generati\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33mon/\u001b[0m\u001b[1;33mutils.py\u001b[0m:\u001b[94m1575\u001b[0m in \u001b[92mgenerate\u001b[0m                                                 \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1572 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                         \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1573 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                          \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1574 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# 13. run sample\u001b[0m                                          \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1575 \u001b[2m│   │   │   \u001b[0mresult = \u001b[96mself\u001b[0m._sample(                                    \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1576 \u001b[0m\u001b[2m│   │   │   │   \u001b[0minput_ids,                                            \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1577 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mlogits_processor=prepared_logits_processor,           \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1578 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mlogits_warper=logits_warper,                          \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/root/miniconda3/envs/llm/lib/python3.11/site-packages/transformers/generati\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33mon/\u001b[0m\u001b[1;33mutils.py\u001b[0m:\u001b[94m2697\u001b[0m in \u001b[92m_sample\u001b[0m                                                  \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m2694 \u001b[0m\u001b[2m│   │   │   \u001b[0mmodel_inputs = \u001b[96mself\u001b[0m.prepare_inputs_for_generation(input_i \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m2695 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                          \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m2696 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# forward pass to get next token\u001b[0m                          \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2697 \u001b[2m│   │   │   \u001b[0moutputs = \u001b[96mself\u001b[0m(                                           \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m2698 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m**model_inputs,                                       \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m2699 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mreturn_dict=\u001b[94mTrue\u001b[0m,                                     \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m2700 \u001b[0m\u001b[2m│   │   │   │   \u001b[0moutput_attentions=output_attentions,                  \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/root/miniconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodu\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[1;33mle.py\u001b[0m:\u001b[94m1511\u001b[0m in \u001b[92m_wrapped_call_impl\u001b[0m                                             \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1508 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._compiled_call_impl \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                      \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1509 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._compiled_call_impl(*args, **kwargs)  \u001b[2m# type:\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1510 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                         \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1511 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._call_impl(*args, **kwargs)                   \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1512 \u001b[0m\u001b[2m│   \u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1513 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_call_impl\u001b[0m(\u001b[96mself\u001b[0m, *args, **kwargs):                            \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1514 \u001b[0m\u001b[2m│   │   \u001b[0mforward_call = (\u001b[96mself\u001b[0m._slow_forward \u001b[94mif\u001b[0m torch._C._get_tracing_s \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/root/miniconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodu\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[1;33mle.py\u001b[0m:\u001b[94m1520\u001b[0m in \u001b[92m_call_impl\u001b[0m                                                     \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1517 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96ms\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1518 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hoo \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1519 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1520 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                      \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1521 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1522 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                          \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1523 \u001b[0m\u001b[2m│   │   │   \u001b[0mresult = \u001b[94mNone\u001b[0m                                             \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/root/miniconda3/envs/llm/lib/python3.11/site-packages/accelerate/\u001b[0m\u001b[1;33mhooks.py\u001b[0m:\u001b[94m1\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[94m66\u001b[0m in \u001b[92mnew_forward\u001b[0m                                                            \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m163 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mwith\u001b[0m torch.no_grad():                                      \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m164 \u001b[0m\u001b[2m│   │   │   │   \u001b[0moutput = module._old_forward(*args, **kwargs)          \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m165 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                          \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m166 \u001b[2m│   │   │   \u001b[0moutput = module._old_forward(*args, **kwargs)              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m167 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m module._hf_hook.post_forward(module, output)            \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m168 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m169 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# Overriding a GraphModuleImpl forward freezes the forward call an\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/root/.cache/huggingface/modules/transformers_modules/\u001b[0m\u001b[1;33mmodeling_chatglm.py\u001b[0m:\u001b[94m94\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[94m1\u001b[0m in \u001b[92mforward\u001b[0m                                                                 \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 938 \u001b[0m\u001b[2m│   │   \u001b[0muse_cache = use_cache \u001b[94mif\u001b[0m use_cache \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[94melse\u001b[0m \u001b[96mself\u001b[0m.conf \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 939 \u001b[0m\u001b[2m│   │   \u001b[0mreturn_dict = return_dict \u001b[94mif\u001b[0m return_dict \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[94melse\u001b[0m \u001b[96msel\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 940 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 941 \u001b[2m│   │   \u001b[0mtransformer_outputs = \u001b[96mself\u001b[0m.transformer(                       \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 942 \u001b[0m\u001b[2m│   │   │   \u001b[0minput_ids=input_ids,                                      \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 943 \u001b[0m\u001b[2m│   │   │   \u001b[0mposition_ids=position_ids,                                \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 944 \u001b[0m\u001b[2m│   │   │   \u001b[0mattention_mask=attention_mask,                            \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/root/miniconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodu\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[1;33mle.py\u001b[0m:\u001b[94m1511\u001b[0m in \u001b[92m_wrapped_call_impl\u001b[0m                                             \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1508 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._compiled_call_impl \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                      \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1509 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._compiled_call_impl(*args, **kwargs)  \u001b[2m# type:\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1510 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                         \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1511 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._call_impl(*args, **kwargs)                   \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1512 \u001b[0m\u001b[2m│   \u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1513 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_call_impl\u001b[0m(\u001b[96mself\u001b[0m, *args, **kwargs):                            \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1514 \u001b[0m\u001b[2m│   │   \u001b[0mforward_call = (\u001b[96mself\u001b[0m._slow_forward \u001b[94mif\u001b[0m torch._C._get_tracing_s \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/root/miniconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodu\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[1;33mle.py\u001b[0m:\u001b[94m1520\u001b[0m in \u001b[92m_call_impl\u001b[0m                                                     \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1517 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96ms\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1518 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hoo \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1519 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1520 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                      \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1521 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1522 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                          \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1523 \u001b[0m\u001b[2m│   │   │   \u001b[0mresult = \u001b[94mNone\u001b[0m                                             \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/root/.cache/huggingface/modules/transformers_modules/\u001b[0m\u001b[1;33mmodeling_chatglm.py\u001b[0m:\u001b[94m81\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[94m1\u001b[0m in \u001b[92mforward\u001b[0m                                                                 \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 808 \u001b[0m\u001b[2m│   │   \u001b[0mbatch_size, seq_length = input_ids.shape                      \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 809 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 810 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m inputs_embeds \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m:                                     \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 811 \u001b[2m│   │   │   \u001b[0minputs_embeds = \u001b[96mself\u001b[0m.embedding(input_ids)                 \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 812 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 813 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.pre_seq_len \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 814 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m past_key_values \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m:                               \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/root/miniconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodu\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[1;33mle.py\u001b[0m:\u001b[94m1511\u001b[0m in \u001b[92m_wrapped_call_impl\u001b[0m                                             \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1508 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._compiled_call_impl \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                      \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1509 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._compiled_call_impl(*args, **kwargs)  \u001b[2m# type:\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1510 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                         \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1511 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._call_impl(*args, **kwargs)                   \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1512 \u001b[0m\u001b[2m│   \u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1513 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_call_impl\u001b[0m(\u001b[96mself\u001b[0m, *args, **kwargs):                            \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1514 \u001b[0m\u001b[2m│   │   \u001b[0mforward_call = (\u001b[96mself\u001b[0m._slow_forward \u001b[94mif\u001b[0m torch._C._get_tracing_s \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/root/miniconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodu\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[1;33mle.py\u001b[0m:\u001b[94m1520\u001b[0m in \u001b[92m_call_impl\u001b[0m                                                     \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1517 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96ms\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1518 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hoo \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1519 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1520 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                      \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1521 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1522 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                          \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1523 \u001b[0m\u001b[2m│   │   │   \u001b[0mresult = \u001b[94mNone\u001b[0m                                             \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/root/.cache/huggingface/modules/transformers_modules/\u001b[0m\u001b[1;33mmodeling_chatglm.py\u001b[0m:\u001b[94m72\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[94m4\u001b[0m in \u001b[92mforward\u001b[0m                                                                 \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 721 \u001b[0m\u001b[2m│   \u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 722 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mforward\u001b[0m(\u001b[96mself\u001b[0m, input_ids):                                     \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 723 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Embeddings.\u001b[0m                                                 \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 724 \u001b[2m│   │   \u001b[0mwords_embeddings = \u001b[96mself\u001b[0m.word_embeddings(input_ids)            \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 725 \u001b[0m\u001b[2m│   │   \u001b[0membeddings = words_embeddings                                 \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 726 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Data format change to avoid explicit tranposes : [b s h] --\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 727 \u001b[0m\u001b[2m│   │   \u001b[0membeddings = embeddings.transpose(\u001b[94m0\u001b[0m, \u001b[94m1\u001b[0m).contiguous()          \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/root/miniconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodu\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[1;33mle.py\u001b[0m:\u001b[94m1511\u001b[0m in \u001b[92m_wrapped_call_impl\u001b[0m                                             \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1508 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._compiled_call_impl \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                      \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1509 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._compiled_call_impl(*args, **kwargs)  \u001b[2m# type:\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1510 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                         \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1511 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._call_impl(*args, **kwargs)                   \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1512 \u001b[0m\u001b[2m│   \u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1513 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_call_impl\u001b[0m(\u001b[96mself\u001b[0m, *args, **kwargs):                            \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1514 \u001b[0m\u001b[2m│   │   \u001b[0mforward_call = (\u001b[96mself\u001b[0m._slow_forward \u001b[94mif\u001b[0m torch._C._get_tracing_s \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/root/miniconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodu\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[1;33mle.py\u001b[0m:\u001b[94m1520\u001b[0m in \u001b[92m_call_impl\u001b[0m                                                     \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1517 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96ms\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1518 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hoo \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1519 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1520 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                      \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1521 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1522 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                          \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m1523 \u001b[0m\u001b[2m│   │   │   \u001b[0mresult = \u001b[94mNone\u001b[0m                                             \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/root/miniconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mspar\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[1;33mse.py\u001b[0m:\u001b[94m163\u001b[0m in \u001b[92mforward\u001b[0m                                                         \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m160 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.weight[\u001b[96mself\u001b[0m.padding_idx].fill_(\u001b[94m0\u001b[0m)                 \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m161 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m162 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mforward\u001b[0m(\u001b[96mself\u001b[0m, \u001b[96minput\u001b[0m: Tensor) -> Tensor:                        \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m163 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m F.embedding(                                            \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m164 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96minput\u001b[0m, \u001b[96mself\u001b[0m.weight, \u001b[96mself\u001b[0m.padding_idx, \u001b[96mself\u001b[0m.max_norm,       \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m165 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.norm_type, \u001b[96mself\u001b[0m.scale_grad_by_freq, \u001b[96mself\u001b[0m.sparse)      \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m166 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/root/miniconda3/envs/llm/lib/python3.11/site-packages/torch/nn/\u001b[0m\u001b[1;33mfunctional.p\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[1;33my\u001b[0m:\u001b[94m2237\u001b[0m in \u001b[92membedding\u001b[0m                                                          \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m2234 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m#   torch.embedding_renorm_\u001b[0m                                   \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m2235 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# remove once script supports set_grad_enabled\u001b[0m                \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m2236 \u001b[0m\u001b[2m│   │   \u001b[0m_no_grad_embedding_renorm_(weight, \u001b[96minput\u001b[0m, max_norm, norm_type \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2237 \u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m torch.embedding(weight, \u001b[96minput\u001b[0m, padding_idx, scale_grad_by_ \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m2238 \u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m2239 \u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m2240 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92membedding_bag\u001b[0m(                                                    \u001b[31m│\u001b[0m\n",
      "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
      "\u001b[1;91mIndexError: \u001b[0mindex out of range in self\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=1  /root/miniconda3/envs/llm/bin/python3 inference_hf.py output/checkpoint-1000/ --prompt \"类型#裙*版型#显瘦*材质#网纱*风格#性感*裙型#百褶*裙下摆#压褶*裙长#连衣裙*裙衣门襟#拉链*裙衣门襟#套头*裙款式#拼接*裙款式#拉链*裙款式#木耳边*裙款式#抽褶*裙款式#不规则\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cd83087f096094",
   "metadata": {},
   "source": [
    "## 4. 总结\n",
    "到此位置，我们就完成了使用单张 GPU Lora 来微调 ChatGLM3-6B 模型，使其能生产出更好的广告。\n",
    "在本章节中，你将会学会：\n",
    "+ 如何使用模型进行 Lora 微调\n",
    "+ 微调数据集的准备和对齐\n",
    "+ 使用微调的模型进行推理"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
